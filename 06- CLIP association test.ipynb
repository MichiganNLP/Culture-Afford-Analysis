{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e44d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import torch\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c14442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose computation device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "# Load pre-trained CLIP model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c0b2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '/'  # Base path for images\n",
    "\n",
    "# Load the CSV file\n",
    "data_unfiltered = pd.read_csv(\"/\".join([path_data, \"images_v2.csv\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98398ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38479"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_unfiltered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc38462",
   "metadata": {},
   "source": [
    "## Remove subjective topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9264be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjective_topics = ['Favorite home decorations', 'Favourite item in kitchen', 'Favourite sports clubs', 'How the most loved item is used', 'icons', 'Idols', 'Latest furniture bought', ' looking over the shoulder', 'Most loved item', 'Most loved toy', 'Most played songs on the radio', 'Music idol', 'Next big thing you are planning to buy', 'Playing with most loved toy', 'Thing I dream about having', 'Things I wish I had', 'Using most loved item', 'Youth culture', 'What I wish I could buy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed98f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_topics = data_unfiltered['topics'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5e7a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Subjective_topic_list = []\n",
    "\n",
    "for topic in unique_topics:\n",
    "    topic_ = topic.replace('/ ', ', ')\n",
    "    topic_split = topic_.split(',')\n",
    "    for split in topic_split:\n",
    "        if split in subjective_topics:\n",
    "            Subjective_topic_list.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a36aaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>country.name</th>\n",
       "      <th>country.id</th>\n",
       "      <th>region.id</th>\n",
       "      <th>type</th>\n",
       "      <th>imageRelPath</th>\n",
       "      <th>topics</th>\n",
       "      <th>place</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5d4bf31ccf0b3a0f3f359814</td>\n",
       "      <td>Burundi</td>\n",
       "      <td>bi</td>\n",
       "      <td>af</td>\n",
       "      <td>image</td>\n",
       "      <td>assets/5d4bf31ccf0b3a0f3f359814/5d4bf31ccf0b3a...</td>\n",
       "      <td>Family snapshots</td>\n",
       "      <td>butoyi</td>\n",
       "      <td>26.994581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d4bf31ccf0b3a0f3f35982a</td>\n",
       "      <td>Burundi</td>\n",
       "      <td>bi</td>\n",
       "      <td>af</td>\n",
       "      <td>image</td>\n",
       "      <td>assets/5d4bf31ccf0b3a0f3f35982a/5d4bf31ccf0b3a...</td>\n",
       "      <td>Cutlery</td>\n",
       "      <td>butoyi</td>\n",
       "      <td>26.994581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d4bf31ccf0b3a0f3f35982e</td>\n",
       "      <td>Burundi</td>\n",
       "      <td>bi</td>\n",
       "      <td>af</td>\n",
       "      <td>image</td>\n",
       "      <td>assets/5d4bf31ccf0b3a0f3f35982e/5d4bf31ccf0b3a...</td>\n",
       "      <td>Family</td>\n",
       "      <td>butoyi</td>\n",
       "      <td>26.994581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d4bf31ccf0b3a0f3f359830</td>\n",
       "      <td>Burundi</td>\n",
       "      <td>bi</td>\n",
       "      <td>af</td>\n",
       "      <td>image</td>\n",
       "      <td>assets/5d4bf31ccf0b3a0f3f359830/5d4bf31ccf0b3a...</td>\n",
       "      <td>Place where eating dinner</td>\n",
       "      <td>butoyi</td>\n",
       "      <td>26.994581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5d4bf31dcf0b3a0f3f35983c</td>\n",
       "      <td>Burundi</td>\n",
       "      <td>bi</td>\n",
       "      <td>af</td>\n",
       "      <td>image</td>\n",
       "      <td>assets/5d4bf31dcf0b3a0f3f35983c/5d4bf31dcf0b3a...</td>\n",
       "      <td>Plate of food</td>\n",
       "      <td>butoyi</td>\n",
       "      <td>26.994581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38474</th>\n",
       "      <td>5ec4f5513f62767d97a47324</td>\n",
       "      <td>France</td>\n",
       "      <td>fr</td>\n",
       "      <td>eu</td>\n",
       "      <td>image</td>\n",
       "      <td>assets/5ec4f5513f62767d97a47324/5ec4f5513f6276...</td>\n",
       "      <td>Bed</td>\n",
       "      <td>larriere</td>\n",
       "      <td>19671.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38475</th>\n",
       "      <td>5ec4f5513f62767d97a47325</td>\n",
       "      <td>France</td>\n",
       "      <td>fr</td>\n",
       "      <td>eu</td>\n",
       "      <td>image</td>\n",
       "      <td>assets/5ec4f5513f62767d97a47325/5ec4f5513f6276...</td>\n",
       "      <td>Bathroom/Toilet</td>\n",
       "      <td>larriere</td>\n",
       "      <td>19671.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38476</th>\n",
       "      <td>5ec4f5523f62767d97a47327</td>\n",
       "      <td>France</td>\n",
       "      <td>fr</td>\n",
       "      <td>eu</td>\n",
       "      <td>image</td>\n",
       "      <td>assets/5ec4f5523f62767d97a47327/5ec4f5523f6276...</td>\n",
       "      <td>Armchair</td>\n",
       "      <td>larriere</td>\n",
       "      <td>19671.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38477</th>\n",
       "      <td>5ec4f5523f62767d97a47328</td>\n",
       "      <td>France</td>\n",
       "      <td>fr</td>\n",
       "      <td>eu</td>\n",
       "      <td>image</td>\n",
       "      <td>assets/5ec4f5523f62767d97a47328/5ec4f5523f6276...</td>\n",
       "      <td>Armchair</td>\n",
       "      <td>larriere</td>\n",
       "      <td>19671.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38478</th>\n",
       "      <td>5ecfdf1f581a3d02cf7f8c05</td>\n",
       "      <td>France</td>\n",
       "      <td>fr</td>\n",
       "      <td>eu</td>\n",
       "      <td>image</td>\n",
       "      <td>assets/5ecfdf1f581a3d02cf7f8c05/5ecfdf1f581a3d...</td>\n",
       "      <td>Family</td>\n",
       "      <td>larriere</td>\n",
       "      <td>19671.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36753 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id country.name country.id region.id   type  \\\n",
       "1      5d4bf31ccf0b3a0f3f359814      Burundi         bi        af  image   \n",
       "2      5d4bf31ccf0b3a0f3f35982a      Burundi         bi        af  image   \n",
       "3      5d4bf31ccf0b3a0f3f35982e      Burundi         bi        af  image   \n",
       "4      5d4bf31ccf0b3a0f3f359830      Burundi         bi        af  image   \n",
       "5      5d4bf31dcf0b3a0f3f35983c      Burundi         bi        af  image   \n",
       "...                         ...          ...        ...       ...    ...   \n",
       "38474  5ec4f5513f62767d97a47324       France         fr        eu  image   \n",
       "38475  5ec4f5513f62767d97a47325       France         fr        eu  image   \n",
       "38476  5ec4f5523f62767d97a47327       France         fr        eu  image   \n",
       "38477  5ec4f5523f62767d97a47328       France         fr        eu  image   \n",
       "38478  5ecfdf1f581a3d02cf7f8c05       France         fr        eu  image   \n",
       "\n",
       "                                            imageRelPath  \\\n",
       "1      assets/5d4bf31ccf0b3a0f3f359814/5d4bf31ccf0b3a...   \n",
       "2      assets/5d4bf31ccf0b3a0f3f35982a/5d4bf31ccf0b3a...   \n",
       "3      assets/5d4bf31ccf0b3a0f3f35982e/5d4bf31ccf0b3a...   \n",
       "4      assets/5d4bf31ccf0b3a0f3f359830/5d4bf31ccf0b3a...   \n",
       "5      assets/5d4bf31dcf0b3a0f3f35983c/5d4bf31dcf0b3a...   \n",
       "...                                                  ...   \n",
       "38474  assets/5ec4f5513f62767d97a47324/5ec4f5513f6276...   \n",
       "38475  assets/5ec4f5513f62767d97a47325/5ec4f5513f6276...   \n",
       "38476  assets/5ec4f5523f62767d97a47327/5ec4f5523f6276...   \n",
       "38477  assets/5ec4f5523f62767d97a47328/5ec4f5523f6276...   \n",
       "38478  assets/5ecfdf1f581a3d02cf7f8c05/5ecfdf1f581a3d...   \n",
       "\n",
       "                          topics     place        income  \n",
       "1               Family snapshots    butoyi     26.994581  \n",
       "2                        Cutlery    butoyi     26.994581  \n",
       "3                         Family    butoyi     26.994581  \n",
       "4      Place where eating dinner    butoyi     26.994581  \n",
       "5                  Plate of food    butoyi     26.994581  \n",
       "...                          ...       ...           ...  \n",
       "38474                        Bed  larriere  19671.000000  \n",
       "38475            Bathroom/Toilet  larriere  19671.000000  \n",
       "38476                   Armchair  larriere  19671.000000  \n",
       "38477                   Armchair  larriere  19671.000000  \n",
       "38478                     Family  larriere  19671.000000  \n",
       "\n",
       "[36753 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data_unfiltered[~data_unfiltered['topics'].isin(Subjective_topic_list)]\n",
    "data = data[data.id != '5d4befbfcf0b3a0f3f353c2e'] #remove rows with corrupted image \n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5d732f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36753"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be5e5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[\"poor\", \"lower-middle\", \"upper-middle\", \"rich\"]\n",
    "\n",
    "#list_of_topic2prompt_dict[1]\n",
    "data[\"quartile\"] = pd.qcut(data[\"income\"], q=[0, 0.25, 0.5, 0.75, 1],    \n",
    "                         labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a026d",
   "metadata": {},
   "source": [
    "## Split topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae631053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45691\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "list_topics = list(data['topics'])\n",
    "\n",
    "\n",
    "separate_topics = [t.lower().strip() for topic in list_topics for t in topic.split(\",\") ]\n",
    "\n",
    "\n",
    "print(len(separate_topics))\n",
    "#print(Counter(separate_topics))\n",
    "set_topics = list(set(separate_topics))\n",
    "print(len(set_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93bbb110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n"
     ]
    }
   ],
   "source": [
    "# map each topic to list of corresponding images\n",
    "dict_topic2img = {}\n",
    "for list_topics, image_path in zip(data['topics'], data['imageRelPath']):\n",
    "    for topic in list_topics.split(\",\"):\n",
    "        topic = topic.lower().strip()\n",
    "        if topic not in dict_topic2img:\n",
    "            dict_topic2img[topic] = set() #### here a set was used in place of list to avoid duplicate where keyword appears twice in a topic\n",
    "        dict_topic2img[topic].add(image_path)\n",
    "\n",
    "print(len(dict_topic2img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f47c29e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45615\n"
     ]
    }
   ],
   "source": [
    "ground_truth_counts = [len(dict_topic2img[i]) for i in dict_topic2img]\n",
    "print(sum(ground_truth_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f5a6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = dict_topic2img.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e11160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mapping_topic2function.pkl\", 'rb') as f:\n",
    "    topic2function = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "151a8233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n"
     ]
    }
   ],
   "source": [
    "dict_function2imgGT = {}\n",
    "\n",
    "for topic in topics:\n",
    "    key = topic2function[topic]\n",
    "    dict_function2imgGT[key] = dict_topic2img[topic]\n",
    "print(len(dict_function2imgGT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7452d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_img2topic, dict_img2country, dict_img2incomelevel, dict_country2region, dict_img2income = {}, {}, {}, {}, {}\n",
    "for list_topics, image_path, country, incomelevel, region, income in zip(data['topics'], data['imageRelPath'], data['country.name'], data['quartile'], data['region.id'], data['income']):\n",
    "    dict_img2topic[image_path] = list_topics\n",
    "    dict_img2country[image_path] = country\n",
    "    dict_img2incomelevel[image_path] = incomelevel\n",
    "    dict_country2region[country] = region\n",
    "    dict_img2income[image_path] = income\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b5650aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_topic2img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2991fb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45615"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list = []\n",
    "for imgs in dict_topic2img.values():\n",
    "    image_list += imgs\n",
    "len(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48c41fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45615"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_list = []\n",
    "for t, imgs in dict_topic2img.items():\n",
    "    topics = [t] * len(imgs)\n",
    "    topic_list += topics\n",
    "len(topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "990232e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45615"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_list = [topic2function[f] for f in topic_list]\n",
    "len(function_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3264643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = [dict_img2country[c] for c in image_list]\n",
    "income_list = [dict_img2incomelevel[i] for i in image_list]\n",
    "region_list = [dict_country2region[cntry] for cntry in country_list]\n",
    "income = [dict_img2income[inc] for inc in image_list]\n",
    "\n",
    "functions = [' '.join(function.split(' ')[5 :]) for function in function_list]\n",
    "function_topic = [f\"the {clean_id} represents {function}\" for clean_id, function in zip(topic_list, functions)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df845e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sep = pd.DataFrame()\n",
    "data_sep['topics'] = topic_list\n",
    "data_sep['image ids'] = image_list\n",
    "data_sep['generated functions'] = function_list\n",
    "data_sep['income level'] = income_list\n",
    "data_sep['income'] = income\n",
    "data_sep['country'] = country_list\n",
    "data_sep['continent'] = region_list\n",
    "data_sep['function_topic'] = function_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ac0b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sep.to_csv('one_image_many_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e6bd7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "016db319",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_images = [path_data + s for s in list(data['image ids'])]\n",
    "image_ids1 = ['/'.join(s.split(\"/\")[8:]) for s in path_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e64bebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5ec4fb77f0611d7ddd742855'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids1[0].split(\"/\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1655047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45615"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e88c7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45615/45615 [00:48<00:00, 934.57it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45615"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path_list = []\n",
    "imgs_corrupted = []\n",
    "for image_path in tqdm(path_images):\n",
    "        path_to_img = image_path\n",
    "        if Path(path_to_img).is_file():\n",
    "            image_path_list.append(path_to_img)\n",
    "        else:\n",
    "            imgs_corrupted.append(path_to_img)\n",
    "len(image_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77b65fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(url_or_path):\n",
    "    if url_or_path.startswith(\"http://\") or url_or_path.startswith(\"https://\"):\n",
    "        return Image.open(requests.get(url_or_path, stream=True).raw)\n",
    "    else:\n",
    "        return Image.open(url_or_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ddb3ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [21:03<3:09:32, 1263.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [44:32<2:59:51, 1348.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [1:07:19<2:38:20, 1357.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [1:31:04<2:18:23, 1383.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [1:38:36<1:27:20, 1048.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [2:01:17<1:16:57, 1154.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [2:23:22<1:00:30, 1210.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [2:43:39<40:24, 1212.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [3:00:13<19:04, 1144.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [3:01:31<00:00, 1089.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image encoding completed..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count = 5000\n",
    "end = len(image_path_list) // count + 1\n",
    "print(end)\n",
    "\n",
    "list_img_emb = []\n",
    "image_names_list = []\n",
    "for k in tqdm(range(0, end)):\n",
    "    print(k)                    \n",
    "    imgs_corrupted = []\n",
    "    images_prep = []\n",
    "    k_images = image_path_list[count*k:count*(k+1)]\n",
    "    print(len(k_images))\n",
    "    \n",
    "\n",
    "    for image_path in k_images:\n",
    "        if Path(image_path).is_file():\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            images_prep.append(preprocess(image))\n",
    "        \n",
    "            \n",
    "    images_prep = torch.stack(images_prep).to(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(images_prep).float()\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        torch.cuda.empty_cache()\n",
    "    list_img_emb.append(image_features)\n",
    "    \n",
    "    image_names = ['/'.join(image_path.split(\"/\")[8:])  for image_path in image_path_list]\n",
    "    image_names_list.append(image_names)\n",
    "    \n",
    "print(\"Image encoding completed..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ebb4790c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45615, 512])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_features = torch.cat(list_img_emb, dim=0)\n",
    "img_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "096ee77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45615, 512])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e80f4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"sep_clip_img_embedding.pkl\",\"wb\")\n",
    "\n",
    "# # write the python object (dict) to pickle file\n",
    "# pickle.dump(img_features,f)\n",
    "\n",
    "# # close file\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4dbf98",
   "metadata": {},
   "source": [
    "### Text and Image Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f942288d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45615, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sep_clip_img_embedding.pkl\",\"rb\") as f:\n",
    "    img_embedding = pickle.load(f)\n",
    "img_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2877b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features = img_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e97a8bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd8c0f",
   "metadata": {},
   "source": [
    "#### Change prompts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8b00f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['function_topic'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f5edeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45615"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bf2d203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "count = 5000\n",
    "end = len(text) // count + 1\n",
    "print(end)\n",
    "score_list = []\n",
    "\n",
    "for k in tqdm(range(10)):\n",
    "    k_img_features = img_features[count*k:count*(k+1)]\n",
    "    k_text = text[count*k:count*(k+1)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_tokens = clip.tokenize(k_text).cuda()\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = text_features.cpu().numpy() @ k_img_features.cpu().numpy().T # dot product of image and text features\n",
    "    similarity_diag = similarity.diagonal()\n",
    "    \n",
    "    score_list += list(similarity_diag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a1c9356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45615"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "387a5894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45615"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00b072fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_1_results = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "095082cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_1_results[\"CLIP score\"] = score_list\n",
    "\n",
    "#exp_1_results = exp_1_results.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ef07655",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_1_results.to_csv('exp_1_sep_function_topic_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6afdf42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>image ids</th>\n",
       "      <th>generated functions</th>\n",
       "      <th>income level</th>\n",
       "      <th>income</th>\n",
       "      <th>country</th>\n",
       "      <th>continent</th>\n",
       "      <th>function_topic</th>\n",
       "      <th>CLIP score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family snapshots</td>\n",
       "      <td>assets/5ec4fb77f0611d7ddd742855/5ec4fb77f0611d...</td>\n",
       "      <td>This is a photo of memories of shared moments.</td>\n",
       "      <td>upper-middle</td>\n",
       "      <td>1394.000000</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>am</td>\n",
       "      <td>the family snapshots represents memories of sh...</td>\n",
       "      <td>0.251733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family snapshots</td>\n",
       "      <td>assets/5d4beb77cf0b3a0f3f34c470/5d4beb77cf0b3a...</td>\n",
       "      <td>This is a photo of memories of shared moments.</td>\n",
       "      <td>rich</td>\n",
       "      <td>3267.973260</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>af</td>\n",
       "      <td>the family snapshots represents memories of sh...</td>\n",
       "      <td>0.266196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family snapshots</td>\n",
       "      <td>assets/5ec4f7f1f0611d7ddd740a82/5ec4f7f1f0611d...</td>\n",
       "      <td>This is a photo of memories of shared moments.</td>\n",
       "      <td>poor</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>as</td>\n",
       "      <td>the family snapshots represents memories of sh...</td>\n",
       "      <td>0.204180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>family snapshots</td>\n",
       "      <td>assets/5d4beaffcf0b3a0f3f34b7bc/5d4beaffcf0b3a...</td>\n",
       "      <td>This is a photo of memories of shared moments.</td>\n",
       "      <td>poor</td>\n",
       "      <td>80.084998</td>\n",
       "      <td>India</td>\n",
       "      <td>as</td>\n",
       "      <td>the family snapshots represents memories of sh...</td>\n",
       "      <td>0.303029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>family snapshots</td>\n",
       "      <td>assets/5d4bf4b0cf0b3a0f3f35c062/5d4bf4b0cf0b3a...</td>\n",
       "      <td>This is a photo of memories of shared moments.</td>\n",
       "      <td>rich</td>\n",
       "      <td>2944.177918</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>as</td>\n",
       "      <td>the family snapshots represents memories of sh...</td>\n",
       "      <td>0.272154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             topics                                          image ids  \\\n",
       "0  family snapshots  assets/5ec4fb77f0611d7ddd742855/5ec4fb77f0611d...   \n",
       "1  family snapshots  assets/5d4beb77cf0b3a0f3f34c470/5d4beb77cf0b3a...   \n",
       "2  family snapshots  assets/5ec4f7f1f0611d7ddd740a82/5ec4f7f1f0611d...   \n",
       "3  family snapshots  assets/5d4beaffcf0b3a0f3f34b7bc/5d4beaffcf0b3a...   \n",
       "4  family snapshots  assets/5d4bf4b0cf0b3a0f3f35c062/5d4bf4b0cf0b3a...   \n",
       "\n",
       "                              generated functions  income level       income  \\\n",
       "0  This is a photo of memories of shared moments.  upper-middle  1394.000000   \n",
       "1  This is a photo of memories of shared moments.          rich  3267.973260   \n",
       "2  This is a photo of memories of shared moments.          poor    96.000000   \n",
       "3  This is a photo of memories of shared moments.          poor    80.084998   \n",
       "4  This is a photo of memories of shared moments.          rich  2944.177918   \n",
       "\n",
       "    country continent                                     function_topic  \\\n",
       "0  Colombia        am  the family snapshots represents memories of sh...   \n",
       "1     Kenya        af  the family snapshots represents memories of sh...   \n",
       "2     Nepal        as  the family snapshots represents memories of sh...   \n",
       "3     India        as  the family snapshots represents memories of sh...   \n",
       "4   Vietnam        as  the family snapshots represents memories of sh...   \n",
       "\n",
       "   CLIP score  \n",
       "0    0.251733  \n",
       "1    0.266196  \n",
       "2    0.204180  \n",
       "3    0.303029  \n",
       "4    0.272154  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_1_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355cb98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c9fedd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
